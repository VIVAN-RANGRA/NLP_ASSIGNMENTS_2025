{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11362297,"sourceType":"datasetVersion","datasetId":7111575}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate bert_score nltk\n!pip install -q git+https://github.com/google-research/bleurt.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:47:44.322019Z","iopub.execute_input":"2025-04-11T13:47:44.322319Z","iopub.status.idle":"2025-04-11T13:49:10.786037Z","shell.execute_reply.started":"2025-04-11T13:47:44.322290Z","shell.execute_reply":"2025-04-11T13:49:10.785166Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ================================\n# Essential Libraries\n# ================================\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\n\n# ================================\n# Hugging Face Transformers & Datasets\n# ================================\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    T5Tokenizer,\n    pipeline,\n    \n)\nfrom torch.optim import AdamW\n\nfrom datasets import load_from_disk\nfrom torch.utils.data import Dataset, DataLoader\n\n# ================================\n# Evaluation & Metrics\n# ================================\nimport evaluate\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom bert_score import score as bert_score\n\n# ================================\n# NLTK Setup (if not downloaded)\n# ================================\nimport nltk\nnltk.download('punkt')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:49:10.788460Z","iopub.execute_input":"2025-04-11T13:49:10.788791Z","iopub.status.idle":"2025-04-11T13:49:38.367033Z","shell.execute_reply.started":"2025-04-11T13:49:10.788766Z","shell.execute_reply":"2025-04-11T13:49:38.366219Z"}},"outputs":[{"name":"stderr","text":"2025-04-11 13:49:22.948464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744379363.178425      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744379363.249464      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Dataset Paths\ndataset_path = \"/kaggle/input/datasetnut/conv_data\"\ntrain_dataset = load_from_disk(os.path.join(dataset_path, \"therapy_train\"))\nval_dataset = load_from_disk(os.path.join(dataset_path, \"therapy_val\"))\ntest_dataset = load_from_disk(os.path.join(dataset_path, \"therapy_test\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:49:38.367830Z","iopub.execute_input":"2025-04-11T13:49:38.368385Z","iopub.status.idle":"2025-04-11T13:49:38.535176Z","shell.execute_reply.started":"2025-04-11T13:49:38.368363Z","shell.execute_reply":"2025-04-11T13:49:38.534599Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ============================================\n# Part 2: Dataset Class for Tokenization\n# ============================================\nclass ConversationDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        input_text = \"predict next utterance: \" + sample[\"input_text\"]\n        target_text = sample[\"target_text\"]\n        inputs = self.tokenizer(input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        targets = self.tokenizer(target_text, max_length=50, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": targets[\"input_ids\"].squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:49:38.535870Z","iopub.execute_input":"2025-04-11T13:49:38.536103Z","iopub.status.idle":"2025-04-11T13:49:38.541898Z","shell.execute_reply.started":"2025-04-11T13:49:38.536082Z","shell.execute_reply":"2025-04-11T13:49:38.541325Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ============================================\n# Part 3: Load Main Models & Tokenizers\n# ============================================\nmodel1_name = \"google/mt5-small\"\nmodel2_name = \"google/flan-t5-base\"\n\nmodel1 = AutoModelForSeq2SeqLM.from_pretrained(model1_name)\ntokenizer1 = AutoTokenizer.from_pretrained(model1_name)\n\nmodel2 = AutoModelForSeq2SeqLM.from_pretrained(model2_name)\ntokenizer2 = AutoTokenizer.from_pretrained(model2_name)\n\nrewriter_model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-small-finetuned-emotion\")\nrewriter_tokenizer = T5Tokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-emotion\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:49:38.542530Z","iopub.execute_input":"2025-04-11T13:49:38.542704Z","iopub.status.idle":"2025-04-11T13:50:02.212442Z","shell.execute_reply.started":"2025-04-11T13:49:38.542689Z","shell.execute_reply":"2025-04-11T13:50:02.211856Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb059378f0b346858f426c8d4df2c8ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d23df83ab39843dbb3c28e8e92287092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"928c1bd3d59f4503b60727d8dbba051a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ca4c540bcbb4ce8b698496ad68f89d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5508a06561834a6788513ace9072806b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a57b7eb9fdf4631ae188bb2dd1a7fb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a63fa4a705d4107aa3bf305835b2909"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e80716d87294bd58fe132b3893d3d76"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7408580ada5149b1bed4418aad5e14d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e730cdc9e138404ab5d136584b7e0759"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"815e90cfaa75469280859b980a6d5860"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a15ac55e59475da0dbdaf5759f8762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7a2c73489342f690119eca0481eb6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e77b4fb516c413f81229f0793e12af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2722ed623d8248948224c686080bfbd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"717c611f0a3e48f38ba6dbe3de1406c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"452a552bb4fb432e8682fdeef1d12bdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98500e271acc4a789df5b9d1bbaac3d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038feede68d842f8bdda55ffe88b74da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9025421704544b079b7654a6273252bd"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ============================================\n# Part 4: Sentiment Detection & Alignment\n# ============================================\nsentiment_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n\ndef get_sentiment(text, max_length=512):\n    # truncate input to max token limit\n    truncated_text = text[:max_length]\n    result = sentiment_classifier(truncated_text)[0][\"label\"].lower()\n    if \"positive\" in result:\n        return 2\n    elif \"negative\" in result:\n        return 0\n    else:\n        return 1\n\n\ndef align_sentiment(prediction, target_sentiment):\n    pred_sentiment = get_sentiment(prediction)\n    if pred_sentiment == target_sentiment:\n        return prediction\n    sentiment_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n    prompt = f\"rephrase to be {sentiment_map[target_sentiment]}: {prediction}\"\n    input_ids = rewriter_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(rewriter_model.device)\n    output_ids = rewriter_model.generate(input_ids, max_length=60, num_beams=5)\n    return rewriter_tokenizer.decode(output_ids[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:02.213245Z","iopub.execute_input":"2025-04-11T13:50:02.213915Z","iopub.status.idle":"2025-04-11T13:50:06.806675Z","shell.execute_reply.started":"2025-04-11T13:50:02.213888Z","shell.execute_reply":"2025-04-11T13:50:06.806092Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d947b3a2222e441eb28d2a8371dcf96c"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed11fc2c3ab545eb9c8546932bc8d37b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b521918b4a4eb9be8d5892db03cc15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cf00fdb863f47c696fbd04a4b0dc193"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================\n# Part 5: Fusion Model with LSTM/GRU Support\n# ============================================\nclass FusionModel(nn.Module):\n    def __init__(self, base_model, fusion=\"lstm\"):\n        super(FusionModel, self).__init__()\n        self.base_model = base_model\n        hidden_size = base_model.config.d_model\n\n        if fusion == \"lstm\":\n            self.fusion_layer = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n            self.linear = nn.Linear(hidden_size * 2, hidden_size)\n        elif fusion == \"gru\":\n            self.fusion_layer = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n            self.linear = nn.Linear(hidden_size * 2, hidden_size)\n        elif fusion == \"lstm_gru\":\n            self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n            self.gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n            self.linear = nn.Linear(hidden_size * 2, hidden_size)\n            self.use_double = True\n        else:\n            raise ValueError(\"Unsupported fusion type\")\n\n        self.use_double = fusion == \"lstm_gru\"\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        encoder = self.base_model.get_encoder()\n        encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n\n        if self.use_double:\n            x, _ = self.lstm(encoder_outputs.last_hidden_state)\n            x, _ = self.gru(x)\n        else:\n            x, _ = self.fusion_layer(encoder_outputs.last_hidden_state)\n\n        reduced_output = self.linear(x)\n\n        outputs = self.base_model(\n            inputs_embeds=reduced_output,\n            attention_mask=attention_mask,\n            labels=labels,\n        )\n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:06.808523Z","iopub.execute_input":"2025-04-11T13:50:06.808750Z","iopub.status.idle":"2025-04-11T13:50:06.815936Z","shell.execute_reply.started":"2025-04-11T13:50:06.808732Z","shell.execute_reply":"2025-04-11T13:50:06.815199Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================\n# Part 6: Training Function\n# ============================================\ndef train_model(model, tokenizer, train_dataset, val_dataset, epochs=10, batch_size=4, lr=5e-5):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    train_loader = DataLoader(ConversationDataset(train_dataset, tokenizer), batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(ConversationDataset(val_dataset, tokenizer), batch_size=batch_size)\n    optimizer = Adam(model.parameters(), lr=lr)\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n            optimizer.zero_grad()\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs.loss\n            total_loss += loss.item()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch+1} Training Loss: {total_loss/len(train_loader):.4f}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                val_loss += outputs.loss.item()\n\n        print(f\"Epoch {epoch+1} Validation Loss: {val_loss/len(val_loader):.4f}\")\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:06.816757Z","iopub.execute_input":"2025-04-11T13:50:06.816945Z","iopub.status.idle":"2025-04-11T13:50:06.835822Z","shell.execute_reply.started":"2025-04-11T13:50:06.816924Z","shell.execute_reply":"2025-04-11T13:50:06.835251Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def generate_response(model, tokenizer, input_text, target_sentiment):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    input_text = \"predict next utterance: \" + input_text\n    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n\n    if hasattr(model, \"generate\"):  # for base T5/MT5 models\n        outputs = model.generate(inputs[\"input_ids\"], max_length=50, num_beams=5)\n    else:  # for FusionModel\n        with torch.no_grad():\n            input_ids = inputs[\"input_ids\"]\n            attention_mask = inputs[\"attention_mask\"]\n\n            encoder = model.base_model.get_encoder()\n            encoder_outputs = encoder(input_ids=input_ids, attention_mask=attention_mask)\n\n            if model.use_double:\n                x, _ = model.lstm(encoder_outputs.last_hidden_state)\n                x, _ = model.gru(x)\n            else:\n                x, _ = model.fusion_layer(encoder_outputs.last_hidden_state)\n\n            reduced_output = model.linear(x)\n\n            decoder_input_ids = torch.tensor([[tokenizer.pad_token_id]]).to(device)\n\n            generated_ids = []\n            for _ in range(50):\n                outputs = model.base_model(\n                    inputs_embeds=reduced_output,\n                    attention_mask=attention_mask,\n                    decoder_input_ids=decoder_input_ids,\n                )\n                next_token_logits = outputs.logits[:, -1, :]\n                next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n                decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=-1)\n                if next_token_id.item() == tokenizer.eos_token_id:\n                    break\n                generated_ids.append(next_token_id.item())\n\n            outputs = torch.tensor(generated_ids).unsqueeze(0).to(device)\n\n    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return align_sentiment(prediction, target_sentiment)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:06.836583Z","iopub.execute_input":"2025-04-11T13:50:06.836832Z","iopub.status.idle":"2025-04-11T13:50:06.857382Z","shell.execute_reply.started":"2025-04-11T13:50:06.836815Z","shell.execute_reply":"2025-04-11T13:50:06.856754Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!pip install -q rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:06.858099Z","iopub.execute_input":"2025-04-11T13:50:06.858352Z","iopub.status.idle":"2025-04-11T13:50:12.651750Z","shell.execute_reply.started":"2025-04-11T13:50:06.858332Z","shell.execute_reply":"2025-04-11T13:50:12.650976Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================\n# Part 8: Evaluation\n# ============================================\nbleurt = evaluate.load(\"bleurt\", checkpoint=\"bleurt-base-128\")\nrouge = evaluate.load(\"rouge\")\n\ndef compute_metrics(preds, refs):\n    bleu_scores = [sentence_bleu([ref.split()], pred.split(), smoothing_function=SmoothingFunction().method1) for pred, ref in zip(preds, refs)]\n    P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=False)\n    rouge_result = rouge.compute(predictions=preds, references=refs)\n    bleurt_score = bleurt.compute(predictions=preds, references=refs)[\"scores\"]\n\n    return {\n        \"BLEU\": sum(bleu_scores)/len(bleu_scores),\n        \"ROUGE-1\": rouge_result[\"rouge1\"],\n        \"ROUGE-2\": rouge_result[\"rouge2\"],\n        \"ROUGE-L\": rouge_result[\"rougeL\"],\n        \"BERTScore F1\": float(F1.mean()),\n        \"BLEURT\": sum(bleurt_score)/len(bleurt_score),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:12.652855Z","iopub.execute_input":"2025-04-11T13:50:12.653163Z","iopub.status.idle":"2025-04-11T13:50:41.155718Z","shell.execute_reply.started":"2025-04-11T13:50:12.653123Z","shell.execute_reply":"2025-04-11T13:50:41.155075Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079b4b374bd14e74ace5e2c9917a1efe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/405M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c55717f8b48a422f853999d12a982750"}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1744379436.496790      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15221 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc42b556c74f4f3ab05162f8fc0df208"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from torch.optim import Adam\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:41.156493Z","iopub.execute_input":"2025-04-11T13:50:41.157051Z","iopub.status.idle":"2025-04-11T13:50:41.161058Z","shell.execute_reply.started":"2025-04-11T13:50:41.157024Z","shell.execute_reply":"2025-04-11T13:50:41.160402Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ============================================\n# Part 9: Model Training & Evaluation\n# ============================================\nmodel1 = train_model(model1, tokenizer1, train_dataset, val_dataset)\nmodel2 = train_model(model2, tokenizer2, train_dataset, val_dataset)\nfusion_m3 = train_model(FusionModel(model1, fusion=\"lstm_gru\"), tokenizer1, train_dataset, val_dataset)\nfusion_m4 = train_model(FusionModel(model2, fusion=\"lstm_gru\"), tokenizer2, train_dataset, val_dataset)\nrefs = [test_dataset[i][\"target_text\"] for i in range(len(test_dataset))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T13:50:41.161781Z","iopub.execute_input":"2025-04-11T13:50:41.161974Z","iopub.status.idle":"2025-04-11T18:43:14.029567Z","shell.execute_reply.started":"2025-04-11T13:50:41.161959Z","shell.execute_reply":"2025-04-11T18:43:14.028777Z"}},"outputs":[{"name":"stderr","text":"Training Epoch 1:   0%|          | 0/1002 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nTraining Epoch 1: 100%|██████████| 1002/1002 [02:51<00:00,  5.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Training Loss: 14.5460\nEpoch 1 Validation Loss: 3.6904\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Training Loss: 3.9905\nEpoch 2 Validation Loss: 2.8317\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 1002/1002 [02:49<00:00,  5.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Training Loss: 3.2522\nEpoch 3 Validation Loss: 2.6117\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Training Loss: 2.8712\nEpoch 4 Validation Loss: 2.1793\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 1002/1002 [02:49<00:00,  5.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Training Loss: 2.5243\nEpoch 5 Validation Loss: 1.9847\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Training Loss: 2.3255\nEpoch 6 Validation Loss: 1.9020\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Training Loss: 2.1594\nEpoch 7 Validation Loss: 1.8298\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Training Loss: 2.0464\nEpoch 8 Validation Loss: 1.7812\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 1002/1002 [02:50<00:00,  5.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Training Loss: 1.9642\nEpoch 9 Validation Loss: 1.7475\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 1002/1002 [02:50<00:00,  5.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Training Loss: 1.8907\nEpoch 10 Validation Loss: 1.7249\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1:   0%|          | 0/1002 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\nTraining Epoch 1: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Training Loss: 2.4220\nEpoch 1 Validation Loss: 1.4599\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Training Loss: 1.4488\nEpoch 2 Validation Loss: 1.4370\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Training Loss: 1.3793\nEpoch 3 Validation Loss: 1.4280\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Training Loss: 1.3175\nEpoch 4 Validation Loss: 1.4292\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 1002/1002 [05:47<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Training Loss: 1.2594\nEpoch 5 Validation Loss: 1.4369\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 1002/1002 [05:46<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Training Loss: 1.1996\nEpoch 6 Validation Loss: 1.4466\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 1002/1002 [05:47<00:00,  2.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Training Loss: 1.1465\nEpoch 7 Validation Loss: 1.4601\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Training Loss: 1.0968\nEpoch 8 Validation Loss: 1.4754\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Training Loss: 1.0467\nEpoch 9 Validation Loss: 1.4994\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 1002/1002 [05:47<00:00,  2.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Training Loss: 1.0006\nEpoch 10 Validation Loss: 1.5247\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Training Loss: 1.9990\nEpoch 1 Validation Loss: 1.8638\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Training Loss: 1.8402\nEpoch 2 Validation Loss: 1.8081\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Training Loss: 1.7908\nEpoch 3 Validation Loss: 1.8015\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Training Loss: 1.7504\nEpoch 4 Validation Loss: 1.7787\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Training Loss: 1.7204\nEpoch 5 Validation Loss: 1.7801\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Training Loss: 1.6930\nEpoch 6 Validation Loss: 1.7590\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Training Loss: 1.6740\nEpoch 7 Validation Loss: 1.7555\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Training Loss: 1.6474\nEpoch 8 Validation Loss: 1.7544\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Training Loss: 1.6299\nEpoch 9 Validation Loss: 1.7229\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 1002/1002 [06:15<00:00,  2.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Training Loss: 1.6105\nEpoch 10 Validation Loss: 1.7352\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 1: 100%|██████████| 1002/1002 [12:57<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 Training Loss: 1.3678\nEpoch 1 Validation Loss: 1.5410\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2: 100%|██████████| 1002/1002 [12:57<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 Training Loss: 1.2289\nEpoch 2 Validation Loss: 1.5418\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3: 100%|██████████| 1002/1002 [12:57<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 Training Loss: 1.1897\nEpoch 3 Validation Loss: 1.5582\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 Training Loss: 1.1532\nEpoch 4 Validation Loss: 1.5778\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5: 100%|██████████| 1002/1002 [12:57<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 Training Loss: 1.1199\nEpoch 5 Validation Loss: 1.5908\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 6: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 Training Loss: 1.0836\nEpoch 6 Validation Loss: 1.5979\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 7: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 Training Loss: 1.0528\nEpoch 7 Validation Loss: 1.6265\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 8: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 Training Loss: 1.0212\nEpoch 8 Validation Loss: 1.6439\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 9: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 Training Loss: 0.9907\nEpoch 9 Validation Loss: 1.6534\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 10: 100%|██████████| 1002/1002 [12:58<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 Training Loss: 0.9637\nEpoch 10 Validation Loss: 1.6773\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================\n# Part 10: Generate and Evaluate All Models\n# ============================================\n\ndef evaluate_model_on_test(model, tokenizer, test_dataset):\n    preds = []\n    for sample in tqdm(test_dataset, desc=\"Generating predictions\"):\n        input_text = sample[\"input_text\"]\n        target_text = sample[\"target_text\"]\n        target_sentiment = get_sentiment(target_text)\n        response = generate_response(model, tokenizer, input_text, target_sentiment)\n        preds.append(response)\n    return compute_metrics(preds, refs)\n\n# Evaluate all models\nresults = {}\n\nresults[\"Model1 (MT5)\"] = evaluate_model_on_test(model1, tokenizer1, test_dataset)\nresults[\"Model2 (T5)\"] = evaluate_model_on_test(model2, tokenizer2, test_dataset)\nresults[\"Model1 + LSTM+GRU\"] = evaluate_model_on_test(fusion_m3, tokenizer1, test_dataset)\nresults[\"Model2 + LSTM+GRU\"] = evaluate_model_on_test(fusion_m4, tokenizer2, test_dataset)\n\n# Display results\nimport pandas as pd\nresults_df = pd.DataFrame(results).T\nprint(\"\\nEvaluation Results:\")\nprint(results_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T19:45:48.409822Z","iopub.execute_input":"2025-04-11T19:45:48.410580Z","iopub.status.idle":"2025-04-11T20:51:30.991694Z","shell.execute_reply.started":"2025-04-11T19:45:48.410552Z","shell.execute_reply":"2025-04-11T20:51:30.990900Z"}},"outputs":[{"name":"stderr","text":"Generating predictions: 100%|██████████| 968/968 [12:23<00:00,  1.30it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nGenerating predictions: 100%|██████████| 968/968 [14:54<00:00,  1.08it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nGenerating predictions: 100%|██████████| 968/968 [20:02<00:00,  1.24s/it]  \nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nGenerating predictions: 100%|██████████| 968/968 [17:35<00:00,  1.09s/it]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluation Results:\n                       BLEU   ROUGE-1   ROUGE-2   ROUGE-L  BERTScore F1  \\\nModel1 (MT5)       0.001159  0.025705  0.003484  0.023650      0.819328   \nModel2 (T5)        0.005368  0.072331  0.014320  0.064172      0.822860   \nModel1 + LSTM+GRU  0.002796  0.053783  0.009155  0.047499      0.824957   \nModel2 + LSTM+GRU  0.002103  0.050307  0.005548  0.045264      0.814564   \n\n                     BLEURT  \nModel1 (MT5)      -1.356782  \nModel2 (T5)       -1.322900  \nModel1 + LSTM+GRU -1.376222  \nModel2 + LSTM+GRU -1.385983  \n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}